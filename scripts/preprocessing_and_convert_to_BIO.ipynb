{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_IIH1zeZGFMF"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import regex\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for NER Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize text using regex pattern and track character offsets. \n",
    "    It returns list of (token, start_offset, end_offset) tuples.\n",
    "    \"\"\"\n",
    "    tokens_with_offsets = []\n",
    "    current_pos = 0\n",
    "    #tokenized_text = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) # pattern to split into words\n",
    "    tokenized_text = regex.split(r'(\\p{P}|\\s+)', text) # pattern to align with BERT Tokenizer\n",
    "    tokenized_text = [item.strip() for item in tokenized_text if item.strip()] # eliminates with spaces\n",
    "    for token in tokenized_text:\n",
    "        if token: \n",
    "            # Find the token in the original text starting from current_pos\n",
    "            start = text.find(token, current_pos)\n",
    "            end = start + len(token)\n",
    "            tokens_with_offsets.append((token, start, end))\n",
    "            current_pos = end\n",
    "    return tokens_with_offsets\n",
    "\n",
    "\n",
    "def assign_entity_labels(tokens_with_offsets, entities, target_entity):\n",
    "    \"\"\"\n",
    "    This function does NOT address overlapping labels and it focuses on one target label at a time.\n",
    "    It returns a list of labels in the form: ['O', 'B-target_entity', 'I-target_entity', 'O', 'O', etc.]\n",
    "    \"\"\"\n",
    "    # Sort entities by start offset\n",
    "    sorted_entities = sorted(entities, key=lambda x: x['start_offset']) \n",
    "    \n",
    "    # Filter entities by target label to only process entities matching the target label\n",
    "    target_entities = [e for e in sorted_entities if e['label'] == target_entity]\n",
    "\n",
    "    # Initialize all tokens with 'O' label\n",
    "    token_labels = ['O'] * len(tokens_with_offsets) \n",
    "\n",
    "    for entity in target_entities:\n",
    "        entity_start, entity_end, entity_label = entity['start_offset'], entity['end_offset'], entity['label']\n",
    "        # Loop through each token and its offsets\n",
    "        for i, (token, token_start, token_end) in enumerate(tokens_with_offsets):\n",
    "            # Skip punctuation tokens\n",
    "            if re.compile(r'^[,.:;?_!\"()\\']$').match(token):\n",
    "                continue\n",
    "            # Check if token overlaps with entity\n",
    "            if (token_start <= entity_end and token_end > entity_start):\n",
    "                # If token starts at or after entity start, mark as B- or I-\n",
    "                if token_start >= entity_start:\n",
    "                    # If it's the first token of the entity, use B- prefix\n",
    "                    if token_start == entity_start:\n",
    "                        token_labels[i] = f'B-{entity_label}'\n",
    "                    else:\n",
    "                        token_labels[i] = f'I-{entity_label}'\n",
    "                # If token starts before entity but overlaps, mark as B-\n",
    "                else:\n",
    "                    token_labels[i] = f'B-{entity_label}'\n",
    "\n",
    "    return token_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Doccano Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '4800_axiomatic_dataset'\n",
    "file_name = '1200_agreement_MC'\n",
    "\n",
    "file_path = f'/home/fantoni/patent-sentence-classification/data/{file_name}.jsonl'\n",
    "label_to_int = {'FUN': 0, 'STR': 1, 'MIX': 2, 'OTH': 3}\n",
    "\n",
    "data = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        json_obj = json.loads(line)\n",
    "        text = json_obj[\"text\"]\n",
    "        entities = json_obj[\"entities\"]\n",
    "        sent, sent_id = re.split(r'\\t', text)\n",
    "        \n",
    "        # Extract Sentence Tag\n",
    "        sent_tags = [entity['label'] for entity in entities if entity['label'] in ['FUN', 'STR', 'MIX', 'OTH']]\n",
    "        # Ensure only one sentence tag per sentence \n",
    "        assert len(sent_tags) == 1, f\"More than one number of sentence tag for sent_id: {sent_id}\"\n",
    "        sent_tag = sent_tags[0] if sent_tags else None\n",
    "        sent_class = label_to_int[sent_tag] # create numeric class\n",
    "\n",
    "        # Extract the tokens eliminating the last token for classification [:-1]\n",
    "        tokens_with_offsets = regex_tokenize(text)\n",
    "        tokens = [t.lower() for t, _ , _ in tokens_with_offsets][:-1]\n",
    "\n",
    "        # Assign lables to tokens eliminating the last token for classification [:-1] \n",
    "        D_labels = assign_entity_labels(tokens_with_offsets, entities, target_entity = 'D')[:-1]\n",
    "        A_labels = assign_entity_labels(tokens_with_offsets, entities, target_entity = 'A')[:-1]\n",
    "        R_labels = assign_entity_labels(tokens_with_offsets, entities, target_entity = 'R')[:-1]\n",
    "        P_labels = assign_entity_labels(tokens_with_offsets, entities, target_entity = 'P')[:-1]\n",
    "        AX_labels = assign_entity_labels(tokens_with_offsets, entities, target_entity = 'AX')[:-1]\n",
    "        \n",
    "        # Ensure all lists are of the same length\n",
    "        assert len(tokens) == len(D_labels) == len(A_labels) == len(R_labels) ==len(P_labels) == len(AX_labels), (\n",
    "            f\"Mismatch in the number of tokens and labels for sent_id: {sent_id}\"\n",
    "        )\n",
    "\n",
    "        # Store data\n",
    "        data.append({\n",
    "            \"sent_id\": sent_id,\n",
    "            \"sent\": sent,\n",
    "            \"sent_tag\": sent_tag,\n",
    "            \"sent_class\": sent_class,\n",
    "            \"words\": '<w>'.join(tokens),\n",
    "            \"D_labels\": ','.join(D_labels),\n",
    "            \"A_labels\": ','.join(A_labels),\n",
    "            \"R_labels\": ','.join(R_labels),\n",
    "            \"P_labels\": ','.join(P_labels),\n",
    "            \"AX_labels\": ','.join(AX_labels)\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found.\n",
      "\n",
      "No missing values found.\n"
     ]
    }
   ],
   "source": [
    "# Perform Sanity Check:\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df[df.duplicated(subset=['sent_id', 'sent'], keep=False)]\n",
    "num_duplicates = duplicates.shape[0]\n",
    "if num_duplicates > 0:\n",
    "    print(\"Duplicate entries:\")\n",
    "    print(duplicates)\n",
    "else:\n",
    "    print('No duplicates found.')\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "if missing_values.sum() > 0:\n",
    "    print(\"\\nMissing values in each column:\")\n",
    "    print(missing_values)\n",
    "    print(\"\\nRows with missing values:\")\n",
    "    print(df[df.isnull().any(axis=1)])\n",
    "else:\n",
    "    print(\"\\nNo missing values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the class label distribution ---> The Dataset is NOT BALANCED!\n",
    "result = df['sent_tag'].value_counts().to_frame(name='count') #print(df['sent_tag'].value_counts())\n",
    "result['%'] = (df['sent_tag'].value_counts(normalize=True) * 100).round(2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame saved to: /home/fantoni/patent-sentence-classification/data/1200_agreement_MC.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Save Dataframe\n",
    "output_file = f'/home/fantoni/patent-sentence-classification/data/{file_name}.xlsx'\n",
    "df.to_excel(output_file, index=False)\n",
    "print(f\"\\nDataFrame saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MC\n",
    "MC_df = pd.read_excel('/home/fantoni/patent-sentence-classification/data/1200_agreement_MC.xlsx')\n",
    "MC_rename_dict = {'sent_tag':'sent_tag_mc','sent_class':'sent_class_mc','words': 'words_mc', 'D_labels': 'D_labels_mc', 'A_labels': 'A_labels_mc', 'R_labels': 'R_labels_mc', 'P_labels': 'P_labels_mc', 'AX_labels': 'AX_labels_mc'}\n",
    "MC_df = MC_df.rename(columns = MC_rename_dict)\n",
    "\n",
    "# Import ML\n",
    "ML_df = pd.read_excel('/home/fantoni/patent-sentence-classification/data/1200_agreement_ML.xlsx')\n",
    "ML_rename_dict = {'sent_tag':'sent_tag_ml','sent_class':'sent_class_ml','words': 'words_ml', 'D_labels': 'D_labels_ml', 'A_labels': 'A_labels_ml', 'R_labels': 'R_labels_ml', 'P_labels': 'P_labels_ml', 'AX_labels': 'AX_labels_ml'}\n",
    "ML_df = ML_df.rename(columns=ML_rename_dict)\n",
    "ML_df = ML_df.drop(columns='sent')\n",
    "\n",
    "# Merge Agreement\n",
    "merged_df = pd.merge(MC_df, ML_df, on=\"sent_id\", how=\"inner\") \n",
    "merged_df['agreement'] = merged_df['sent_class_mc'] == merged_df['sent_class_ml']\n",
    "\n",
    "# Save\n",
    "merged_df.to_excel('/home/fantoni/patent-sentence-classification/data/1200_agreement_All.xlsx', index= False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPaM7lcGR+NnrzmqsyZBGAQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
