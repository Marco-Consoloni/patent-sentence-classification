{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import warnings \n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tqdm import tqdm  \n",
    "from transformers import AutoTokenizer\n",
    "#from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The installation of dotenv failed for the container, I need to explicitly set the hf token.\n",
    "#load_dotenv()\n",
    "#token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "token = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Using Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Set model name\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "#model_id = \"meta-llama/Meta-Llama-3-70B\"\n",
    "\n",
    "text_generator = transformers.pipeline(\"text-generation\", # LLM task\n",
    "                                 model=model_id,\n",
    "                                 model_kwargs={\"torch_dtype\": torch.float16},\n",
    "                                 device=device,\n",
    "                                 token=token,\n",
    "                                 )\n",
    "                                 \n",
    "print(\"Model loaded successfully using pipline.\")\n",
    "\n",
    "# Prompt\n",
    "prompt = \"Once upon a time, in a faraway kingdom,\"\n",
    "\n",
    "# Generate response\n",
    "response = text_generator(prompt, max_length=100, num_return_sequences=1)\n",
    "\n",
    "# Print the generated text\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Llama-3.1-8B-Instruct loaded successfully!\n",
      "Tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Set model name\n",
    "#model_id = \"meta-llama/Meta-Llama-3-70B\" # too big to run on GPU\n",
    "#model_id = \"meta-llama/Llama-3.3-70B-Instruct\" # too big to run on GPU\n",
    "#model_id = \"meta-llama/Meta-Llama-3.1-8B\"; model_name = 'Llama-3.1-8B'\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"; model_name = 'Llama-3.1-8B-Instruct'\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    local_files_only=True,  # Ensure local loading\n",
    "    token=token\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "model.to(device)\n",
    "print(f\"Model {model_name} loaded successfully!\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    local_files_only=True,\n",
    "    token=token, \n",
    "    device=device)\n",
    "print(\"Tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a faraway kingdom, there lived a beautiful princess named Sophia. Sophia was known throughout the kingdom for her kindness, intelligence, and bravery. She was loved by all who knew her, and her parents, the king and queen, were extremely proud of their daughter.\n",
      "One day, a wicked sorcerer cast a spell on the kingdom, causing a terrible drought to afflict the land. The crops began to wither and die, and the people were in dire need of\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "prompt = \"Once upon a time, in a faraway kingdom,\"\n",
    "\n",
    "# Tokenize and generate response\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "output = model.generate(**inputs, max_length=100, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Decode and print result\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Classification Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentence_llama(input_text, model, tokenizer, template, device):\n",
    "\n",
    "    # Prepare prompt \n",
    "    prompt = template.format(text=input_text, class_labels=\"'FUN', 'STR', 'MIX' or 'OTH'\")\n",
    "     \n",
    "    # Tokenize and generate response\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(**inputs, max_new_tokens=5, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # Decode and print result\n",
    "    output_string = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Use regex to match the sentence class (since prompt is repeated in the output string)\n",
    "    match = re.search(r\"Class:\\s*('?)(FUN|STR|MIX|OTH)('?)\", output_string, re.IGNORECASE)\n",
    "    # Extract label if matched, otherwise default to 'UNK'\n",
    "    output_label = match.group(2).upper() if match else None\n",
    "\n",
    "    # Token count (excluding padding)\n",
    "    input_count = len(tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0])\n",
    "    output_count = len(output[0])\n",
    "\n",
    "    # Debugging statement (prints the formatted prompt)\n",
    "    #print(f\"Generated Prompt:\\n{prompt}\")\n",
    "\n",
    "    return output_string, output_label, input_count, output_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output String: \n",
      "        Your task is to classify a given sentence as either: \n",
      "        * 'FUN' - if the sentence describes only the functioning or behavior of a device;\n",
      "        * 'STR' - if the sentence describes only the structure or architecture of a device;\n",
      "        * 'MIX' - if the sentence describes both the functioning and the structure of a device;\n",
      "        * 'OTH' - if the sentence cannot be classified according to any of the previous classes.\n",
      "        \n",
      "        The output should only contain one of the class labels: 'FUN', 'STR', 'MIX' or 'OTH'.\n",
      "        Sentence: \"Both Extracts 1-3 and 6-9 samples and standards were diluted 1:40 in the provided dilution buffer (for example 15 mL serum+585 mL buffer).\"\n",
      "        Class:\n",
      "        \"OTH\"\n",
      "       \n",
      "Input Tokens: 167\n",
      "Output Tokens: 173\n",
      "Predicted Class: None\n"
     ]
    }
   ],
   "source": [
    "# Sample Sentences for Testing\n",
    "input_text = \"Additionally, the stopper 108 is used at the distal end of the wire where the loop is formed to substantially secure the loop closed.\" # MIX\n",
    "input_text = \"Provisional Patent Application number 62/571,193; filed Oct. 11, 2017; and entitled INSECT VACUUM AND TRAP ATTACHMENT SYSTEMS.\" #OTH\n",
    "input_text = \"In some embodiments, the horizontal position of the idler support block 1213 may be adjustable to maintain tension on the chain 1212.\" #FUN\n",
    "input_text = \"If there are no allocated cells to a hub using the previous criterion, the first allocated cell will be the closest cell to that hub.\" #FUN\n",
    "input_text = \"The rigid foam layer 50 is typically selected from the group of polyurethane foams, polyurea foams, and combinations thereof.\" # STR\n",
    "input_text = \"Both Extracts 1-3 and 6-9 samples and standards were diluted 1:40 in the provided dilution buffer (for example 15 mL serum+585 mL buffer).\" #FUN\n",
    "\n",
    "prompt = f\"\"\"\n",
    "        Your task is to classify a given sentence as either: \n",
    "        * 'FUN' - if the sentence describes only the functioning or behavior of a device;\n",
    "        * 'STR' - if the sentence describes only the structure or architecture of a device;\n",
    "        * 'MIX' - if the sentence describes both the functioning and the structure of a device;\n",
    "        * 'OTH' - if the sentence cannot be classified according to any of the previous classes.\n",
    "        \n",
    "        The output should only contain one of the class labels: 'FUN', 'STR', 'MIX' or 'OTH'.\n",
    "        Sentence: \"{input_text}\"\n",
    "        Class:\n",
    "\"\"\"\n",
    "\n",
    "output_string, output_label, input_count, output_count = classify_sentence_llama(input_text=input_text, model=model, tokenizer=tokenizer, template=prompt, device=device)\n",
    "print(f\"Output String: {output_string}\")\n",
    "print(f\"Input Tokens: {input_count}\")\n",
    "print(f\"Output Tokens: {output_count}\")\n",
    "print(f\"Predicted Class: {output_label}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/workspace/prompting/prompt_templates.json\", \"r\") as file:\n",
    "        templates = json.load(file)\n",
    "\n",
    "# Vizualize templates\n",
    "#for name, template in templates.items():\n",
    "#    print(f\"{name}:\")\n",
    "#    print(template)\n",
    "#    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Classify Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "test_df = pd.read_excel(\"/workspace/data/test_agreement.xlsx\")\n",
    "#test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1200/1200 [06:15<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt1 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1200/1200 [06:18<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt3 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1200/1200 [06:20<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt5 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1200/1200 [06:27<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt8 completed.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over templates\n",
    "for templ_name, template in templates.items():\n",
    "    if templ_name in ['prompt1', 'prompt3', 'prompt5', 'prompt8']:  # 'prompt3','prompt5', 'prompt8' choose and skip specific templates\n",
    "    \n",
    "        # Initialize results dictionary\n",
    "        results = {\n",
    "            'sent_id': [],\n",
    "            'sent': [],\n",
    "            'true_class': [],\n",
    "            'pred_class': [],\n",
    "            'model_output': [],\n",
    "            'input_count': [],\n",
    "            'output_count': [],\n",
    "            'errors': [],\n",
    "            'elapsed_time_sec': []\n",
    "        }\n",
    "\n",
    "        # Perform Classification\n",
    "        for i, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing Sentences\"):  \n",
    "            start_time = time.time()              \n",
    "            try:\n",
    "                results['sent_id'].append(row['sent_id'])\n",
    "                results['sent'].append(row['sent'])\n",
    "                results['true_class'].append(row['sent_tag'])\n",
    "\n",
    "                # Get classification\n",
    "                output_string, output_label, input_count, output_count = classify_sentence_llama(input_text=row['sent'], model=model, tokenizer=tokenizer, template=template, device=device)\n",
    "\n",
    "                # Validate classification output\n",
    "                if output_label not in [\"FUN\", \"STR\", \"MIX\", \"OTH\"]:\n",
    "                    raise ValueError(f\"Invalid Classification Tag.\")\n",
    "\n",
    "                # Append Results \n",
    "                results['pred_class'].append(output_label)\n",
    "                results['model_output'].append(output_string)\n",
    "                results['errors'].append(None)  # No error occurred\n",
    "                results['input_count'].append(input_count)  # Append input token count when successful\n",
    "                results['output_count'].append(output_count) # Append output token count when successful\n",
    "\n",
    "            except Exception as e:\n",
    "                # Handle any errors that occur during processing\n",
    "                print(f\"Error in sentence {row['sent_id']}: {e}. Skipping...\")\n",
    "                results['pred_class'].append(\"ERROR\")\n",
    "                results['model_output'].append(output_string)\n",
    "                results['input_count'].append(\"ERROR\") \n",
    "                results['output_count'].append(\"ERROR\") \n",
    "                results['errors'].append(str(e))\n",
    "            \n",
    "            finally:\n",
    "                # Tracking how long it took to process the sentence\n",
    "                end_time = time.time()\n",
    "                elapsed_time = end_time-start_time\n",
    "                results['elapsed_time_sec'].append(f\"{elapsed_time}\")\n",
    "\n",
    "        # Convert results to DataFrame and Save\n",
    "        result_df = pd.DataFrame(results)\n",
    "        result_df.to_excel(f'/workspace/results/prompting/{model_name}_{templ_name}.xlsx', index = False)\n",
    "        print(f\"{templ_name} completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
