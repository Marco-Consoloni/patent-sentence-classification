{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify with Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install --upgrade pip\n",
    "#!pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "#from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The installation of dotenv failed for the container, I need to explicitly set the hf token.\n",
    "#load_dotenv()\n",
    "#token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "token = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Using Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [06:21<00:00, 95.49s/it]\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully on GPU!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Set model name\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "#model_id = \"meta-llama/Meta-Llama-3-70B\"\n",
    "\n",
    "text_generator = transformers.pipeline(\"text-generation\", # LLM task\n",
    "                                 model=model_id,\n",
    "                                 model_kwargs={\"torch_dtype\": torch.float16},\n",
    "                                 device=device,\n",
    "                                 token=token,\n",
    "                                 )\n",
    "                                 \n",
    "print(\"Model loaded successfully using pipline.\")\n",
    "\n",
    "# Prompt\n",
    "prompt = \"Once upon a time, in a faraway kingdom,\"\n",
    "\n",
    "# Generate response\n",
    "response = text_generator(prompt, max_length=100, num_return_sequences=1)\n",
    "\n",
    "# Print the generated text\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully loaded from local cache!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'What is the meaning of life? This is a question that has been pondered by philosophers, theologians, and scientists for centuries. There is no one answer that is universally accepted, but there are many different perspectives on the matter. Some people believe that life has no meaning, while others believe that it is up to each individual to find their own meaning. There are also those who believe that life has a specific purpose or meaning, which is determined by a higher power or by nature itself.\\n'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Set model name\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "model_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "#model_id = \"meta-llama/Meta-Llama-3-70B\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    local_files_only=True,  # Ensure local loading\n",
    "    device_map=device,  # Automatically allocate to available GPUs\n",
    "    token=token\n",
    ")\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, local_files_only=True, token=token)\n",
    "print(\"Tokenizer loaded successfully!\")\n",
    "\n",
    "# Prompt\n",
    "prompt = \"Once upon a time, in a faraway kingdom,\"\n",
    "\n",
    "# Tokenize and generate response\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "output = model.generate(**inputs, max_length=100)\n",
    "\n",
    "# Decode and print result\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
