{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import warnings \n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tqdm import tqdm  \n",
    "from transformers import AutoTokenizer\n",
    "#from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The installation of dotenv failed for the container, I need to explicitly set the hf token.\n",
    "#load_dotenv()\n",
    "#token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "token = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Using Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Set model name\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "#model_id = \"meta-llama/Meta-Llama-3-70B\"\n",
    "\n",
    "text_generator = transformers.pipeline(\"text-generation\", # LLM task\n",
    "                                 model=model_id,\n",
    "                                 model_kwargs={\"torch_dtype\": torch.float16},\n",
    "                                 device=device,\n",
    "                                 token=token,\n",
    "                                 )\n",
    "                                 \n",
    "print(\"Model loaded successfully using pipline.\")\n",
    "\n",
    "# Prompt\n",
    "prompt = \"Once upon a time, in a faraway kingdom,\"\n",
    "\n",
    "# Generate response\n",
    "response = text_generator(prompt, max_length=100, num_return_sequences=1)\n",
    "\n",
    "# Print the generated text\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Loading\n",
    "\n",
    "see llama family: https://huggingface.co/meta-llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'Llama-3.1-8B-Instruct' loaded successfully!\n",
      "Tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Set model name\n",
    "#model_id = \"meta-llama/Meta-Llama-3-70B\" # too big to run on GPU\n",
    "#model_id = \"meta-llama/Llama-3.3-70B-Instruct\" # too big to run on GPU\n",
    "#model_id = \"meta-llama/Meta-Llama-3.1-8B\"; model_name = 'Llama-3.1-8B'\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"; model_name = 'Llama-3.1-8B-Instruct'\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    local_files_only=True,  # Ensure local loading\n",
    "    token=token\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "model.to(device)\n",
    "print(f\"Model '{model_name}' loaded successfully!\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    local_files_only=True,\n",
    "    token=token, \n",
    "    device=device)\n",
    "print(\"Tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a faraway kingdom, there lived a beautiful princess named Sophia. She had long, golden hair and sparkling blue eyes that shone like the stars in the night sky. Sophia was kind and gentle, loved by all who knew her, and she spent her days helping those in need and spreading joy wherever she went.\n",
      "One day, a wicked sorcerer cast a spell on the kingdom, causing all the flowers to wither and die. The kingdom's gardens, once\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "prompt = \"Once upon a time, in a faraway kingdom,\"\n",
    "\n",
    "# Tokenize and generate response\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "output = model.generate(**inputs, max_length=100, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Decode and print result\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Classification Function\n",
    "\n",
    "reference: https://huggingface.co/docs/transformers/main_classes/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentence_llama(input_text, model, tokenizer, template, device):\n",
    "\n",
    "    # Format the prompt using the provided template \n",
    "    prompt = template.format(text=input_text, class_labels=\"'FUN', 'STR', 'MIX' or 'OTH'\")\n",
    "\n",
    "    # Tokenize the formatted prompt to determine the starting position of the response\n",
    "    # This is needed because some LLaMA models tend to repeat parts of the prompt in their output\n",
    "    # See: https://stackoverflow.com/questions/76772509/llama-2-7b-hf-repeats-context-of-question-directly-from-input-prompt-cuts-off-w\n",
    "    prompt_tokens = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    start_index = prompt_tokens.shape[-1]\n",
    "     \n",
    "    # Convert the prompt into tokenized input format and move it to the appropriate device (CPU/GPU)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate output using the model\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=5,\n",
    "        num_return_sequences=1, # Generate only one response\n",
    "        do_sample=False, # Use deterministic output instead of sampling\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Extract the model-generated text excluding the prompt itself\n",
    "    output_without_prompt = output[0][start_index:]\n",
    "\n",
    "    # Decode the generated tokens into human-readable text\n",
    "    #output_string_completed = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    output_string = tokenizer.decode(output_without_prompt, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Use regex to match the sentence class (since prompt is repeated in the output string)\n",
    "    match = re.search(r\"(['\\\"]?)(FUN|STR|MIX|OTH)(['\\\"]?)\", output_string, re.IGNORECASE)\n",
    "    # Extract label if matched, otherwise default to 'UNK'\n",
    "    output_label = match.group(2).upper() if match else None\n",
    "\n",
    "    # Token count (excluding padding)\n",
    "    input_count = len(tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0])\n",
    "    output_count = len(output[0])\n",
    "\n",
    "    # Debugging statement (prints the formatted prompt)\n",
    "    #print(f\"Generated Prompt:\\n{prompt}\")\n",
    "\n",
    "    return output_string, output_label, input_count, output_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/workspace/prompting/prompt_templates.json\", \"r\") as file:\n",
    "        templates = json.load(file)\n",
    "\n",
    "# Vizualize templates\n",
    "for name, template in templates.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(template)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Classification Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your task is to classify a given sentence as either: \n",
      "* 'FUN' - if the sentence describes only the functioning or behavior of a device;\n",
      "* 'STR' - if the sentence describes only the structure or architecture of a device;\n",
      "* 'MIX' - if the sentence describes both the functioning and the structure of a device;\n",
      "* 'OTH' - if the sentence cannot be classified according to any of the previous classes.\n",
      "The output should only contain one of the class labels: FUN, STR, MIX or OTH.\n",
      "Sentence: '{text}' \n",
      "Class: \n",
      "\n",
      "\n",
      "*Output String*: 'OTH' \n",
      "Explanation\n",
      "\n",
      "*Input Tokens*: 152\n",
      "\n",
      "*Output Tokens*: 158\n",
      "\n",
      "*Predicted Class*: OTH\n"
     ]
    }
   ],
   "source": [
    "# Sample Sentences for Testing\n",
    "input_text = \"Additionally, the stopper 108 is used at the distal end of the wire where the loop is formed to substantially secure the loop closed.\" # MIX\n",
    "input_text = \"Provisional Patent Application number 62/571,193; filed Oct. 11, 2017; and entitled INSECT VACUUM AND TRAP ATTACHMENT SYSTEMS.\" #OTH\n",
    "input_text = \"In some embodiments, the horizontal position of the idler support block 1213 may be adjustable to maintain tension on the chain 1212.\" #FUN\n",
    "input_text = \"If there are no allocated cells to a hub using the previous criterion, the first allocated cell will be the closest cell to that hub.\" #FUN\n",
    "input_text = \"The rigid foam layer 50 is typically selected from the group of polyurethane foams, polyurea foams, and combinations thereof.\" # STR\n",
    "input_text = \"Both Extracts 1-3 and 6-9 samples and standards were diluted 1:40 in the provided dilution buffer (for example 15 mL serum+585 mL buffer).\" #FUN\n",
    "\n",
    "# Select Template\n",
    "template = templates['prompt_1']\n",
    "print(template, '\\n')\n",
    "\n",
    "output_string, output_label, input_count, output_count = classify_sentence_llama(input_text=input_text, model=model, tokenizer=tokenizer, template=template, device=device)\n",
    "print(f\"\\n*Output String*: {output_string}\")\n",
    "print(f\"\\n*Input Tokens*: {input_count}\")\n",
    "print(f\"\\n*Output Tokens*: {output_count}\")\n",
    "print(f\"\\n*Predicted Class*: {output_label}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Classify Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "test_df = pd.read_excel(\"/workspace/data/test_agreement.xlsx\")\n",
    "#test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences: 100%|███████████████████████████████████████████████████████████████████████| 1200/1200 [07:34<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_1 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences: 100%|███████████████████████████████████████████████████████████████████████| 1200/1200 [07:36<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_2 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences: 100%|███████████████████████████████████████████████████████████████████████| 1200/1200 [07:37<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_3 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences: 100%|███████████████████████████████████████████████████████████████████████| 1200/1200 [07:40<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_4 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences: 100%|███████████████████████████████████████████████████████████████████████| 1200/1200 [07:37<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_5 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences: 100%|███████████████████████████████████████████████████████████████████████| 1200/1200 [07:43<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_6 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences: 100%|███████████████████████████████████████████████████████████████████████| 1200/1200 [07:43<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_7 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences: 100%|███████████████████████████████████████████████████████████████████████| 1200/1200 [07:41<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_8 completed.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over templates\n",
    "for templ_name, template in templates.items():\n",
    "    if templ_name not in ['prompt_x']:  # 'prompt1', 'prompt3', 'prompt5', 'prompt8' choose and skip specific templates\n",
    "    \n",
    "        # Initialize results dictionary\n",
    "        results = {\n",
    "            'sent_id': [],\n",
    "            'sent': [],\n",
    "            'sent_tag': [],\n",
    "            'predicted_tag': [],\n",
    "            'model_output': [],\n",
    "            'input_count': [],\n",
    "            'output_count': [],\n",
    "            'errors': [],\n",
    "            'elapsed_time_sec': []\n",
    "        }\n",
    "\n",
    "        # Perform Classification\n",
    "        for i, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing Sentences\"):  \n",
    "            start_time = time.time()              \n",
    "            try:\n",
    "                results['sent_id'].append(row['sent_id'])\n",
    "                results['sent'].append(row['sent'])\n",
    "                results['sent_tag'].append(row['sent_tag'])\n",
    "\n",
    "                # Get classification\n",
    "                output_string, output_label, input_count, output_count = classify_sentence_llama(input_text=row['sent'], model=model, tokenizer=tokenizer, template=template, device=device)\n",
    "\n",
    "                # Validate classification output\n",
    "                if output_label not in [\"FUN\", \"STR\", \"MIX\", \"OTH\"]:\n",
    "                    raise ValueError(f\"Invalid Classification Tag.\")\n",
    "\n",
    "                # Append Results \n",
    "                results['predicted_tag'].append(output_label)\n",
    "                results['model_output'].append(output_string)\n",
    "                results['errors'].append(None)  # No error occurred\n",
    "                results['input_count'].append(input_count)  # Append input token count when successful\n",
    "                results['output_count'].append(output_count) # Append output token count when successful\n",
    "\n",
    "            except Exception as e:\n",
    "                # Handle any errors that occur during processing\n",
    "                print(f\"Error in sentence {row['sent_id']}: {e}. Skipping...\")\n",
    "                results['predicted_tag'].append(\"ERROR\")\n",
    "                results['model_output'].append(output_string)\n",
    "                results['input_count'].append(\"ERROR\") \n",
    "                results['output_count'].append(\"ERROR\") \n",
    "                results['errors'].append(str(e))\n",
    "            \n",
    "            finally:\n",
    "                # Tracking how long it took to process the sentence\n",
    "                end_time = time.time()\n",
    "                elapsed_time = end_time-start_time\n",
    "                results['elapsed_time_sec'].append(f\"{elapsed_time}\")\n",
    "\n",
    "        # Convert results to DataFrame and Save\n",
    "        result_df = pd.DataFrame(results)\n",
    "        result_df.to_excel(f'/workspace/results/prompting/{model_name}_{templ_name}.xlsx', index = False)\n",
    "        print(f\"{templ_name} completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
