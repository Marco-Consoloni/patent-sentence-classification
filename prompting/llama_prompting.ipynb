{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify with Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install --upgrade pip\n",
    "#!pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      7\u001b[0m load_dotenv()\n\u001b[1;32m      8\u001b[0m token \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHUGGINGFACE_TOKEN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "token = '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'\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "#model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "#model_id = \"meta-llama/Meta-Llama-3-70B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=token)\n",
    "print(\"Tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [06:21<00:00, 95.49s/it]\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully on GPU!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipeline = transformers.pipeline(\"text-generation\", # LLM task\n",
    "                                 model=model_id,\n",
    "                                 model_kwargs={\"torch_dtype\": torch.float16},\n",
    "                                 device=device,\n",
    "                                 token=token,\n",
    "                                 )\n",
    "\n",
    "print(\"Model loaded successfully on GPU!\" if device == \"cuda\" else \"Model running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract all the verbs from the following sentence. \n",
      "Sentence: \"a user may actuate the latch 300 by providing a force to the latch 300 directed away from the seat plate 204.\". \n",
      "Return only the list of verbs.\n"
     ]
    }
   ],
   "source": [
    "prompt_original = \"Extract the verbs from the following sentence. \\nReturn a list of verbs formatted as ['verb1', 'verb2', 'verb3', and so on]. If no verbs are present within the sentence, return []. \\nSentence: \\\"a user may actuate the latch 300 by providing a force to the latch 300 directed away from the seat plate 204.\\\" \\nOutput:\"\n",
    "prompt_no_line_breaks = \"Extract the verbs from the following sentence. Output a list of verbs formatted as ['verb1', 'verb2', 'verb3', and so on]. If no verbs are present within the sentence, the output is []. Sentence: \\\"a user may actuate the latch 300 by providing a force to the latch 300 directed away from the seat plate 204.\\\" Output:\"\n",
    "prompt_base = \"Extract all the verbs from the following sentence. \\nSentence: \\\"a user may actuate the latch 300 by providing a force to the latch 300 directed away from the seat plate 204.\\\". \\nReturn only the list of verbs.\"\n",
    "print(prompt_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'generated_text': 'Extract all the verbs from the following sentence. \\nSentence: \"a user may actuate the latch 300 by providing a force to the latch 300 directed away from the seat plate 204.\". \\nReturn only the list of verbs. \\n'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = pipeline(prompt_base)\n",
    "response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract all the verbs from the following sentence. \n",
      "Sentence: \"a user may actuate the latch 300 by providing a force to the latch 300 directed away from the seat plate 204.\". \n",
      "Return only the list of verbs. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract the verbs from the following sentence. Output a list of verbs formatted as ['verb1', 'verb2', 'verb3', and so on]. If no verbs are present within the sentence, the output is []. Sentence: \"a user may actuate the latch 300 by providing a force to the latch 300 directed away from the seat plate 204.\" Output: ['actuate']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_1 = pipeline(prompt_no_line_breaks)\n",
    "print(response_1[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully loaded from local cache!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'What is the meaning of life? This is a question that has been pondered by philosophers, theologians, and scientists for centuries. There is no one answer that is universally accepted, but there are many different perspectives on the matter. Some people believe that life has no meaning, while others believe that it is up to each individual to find their own meaning. There are also those who believe that life has a specific purpose or meaning, which is determined by a higher power or by nature itself.\\n'}]\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import os\n",
    "\n",
    "# Define the model ID\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, \n",
    "        torch_dtype=torch.float16, \n",
    "        local_files_only=True  # Force loading from local cache\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        local_files_only=True  # Prevent re-downloading\n",
    "    )\n",
    "print(\"Model successfully loaded from local cache!\")\n",
    "\n",
    "# Create a text generation pipeline\n",
    "text_gen_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if device == \"cuda\" else -1\n",
    ")\n",
    "\n",
    "\n",
    "# Test the pipeline\n",
    "output = text_gen_pipeline(\"What is the meaning of life?\", max_length=100)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a wrapper function over the method pipeline()\n",
    "def get_llama_response(prompt: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate a response from the Llama model.\n",
    "    Parameters:\n",
    "        prompt (str): The user's input/question for the model.\n",
    "    Returns:\n",
    "        None: Prints the model's response.\n",
    "    \"\"\"\n",
    "    sequences = pipeline(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=50,\n",
    "    )\n",
    "    print(\"Chatbot:\", sequences[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Extract all the verbs from the following sentence. \n",
      "Sentence: \"a user may actuate the latch 300 by providing a force to the latch 300 directed away from the seat plate 204.\". \n",
      "Return only the list of verbs. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_llama_response(prompt_base)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
